<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="style.css">

    <title>Georgios Georgakis</title>
    
  </head>

  <body>
    <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Georgios Georgakis
                </p>
                <p>
I am a Robotics Technologist at the <a href="https://www.jpl.nasa.gov/">NASA-Jet Propulsion Laboratory</a>. 
My research interests lie at the intersection of computer vision and machine learning, with applications in robotics.
My long-term goal is enabling the exploration and understanding of our solar system by developing generalizable, 
learning-based perception and navigation algorithms for robotic systems,
and by designing AI models that maximize the scientific return from space mission data.
<br><br>
<!-- At JPL, I have been developing perception algorithms in support of multiple autonomous systems, including a future Mars Science Helicopter. -->
Prior to joining JPL, I was a Postdoctoral Researcher in the <a href="https://www.grasp.upenn.edu/">GRASP Lab</a> at the University of Pennsylvania, advised 
by <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>. During my PostDoc I worked on vision-based semantic navigation in novel environments
for the tasks of exploration, object-goal navigation, and vision and language navigation (VLN).
I received my Ph.D from the Department of <a href="http://cs.gmu.edu/">Computer Science</a> 
at <a href="https://www2.gmu.edu/">George Mason University</a>, advised by <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a>.
I completed my Diploma in Computer Engineering from the <a href="http://www.tuc.gr/4992.html">
Technical University of Crete</a> in Greece, where I worked with Dr. <a href="http://www.intelligence.tuc.gr/~lagoudakis/">Michail G. Lagoudakis</a> on 
landmark recognition and localization for robotic soccer.
<!-- During my Ph.D I worked on multiple problems such as object detection and recognition
in RGB-D data, keypoint and descriptor learning, and 3D object pose estimation.--> 
<br><br>
I spent time as a research intern at <a href="https://www.siemens.com/us/en.html">Siemens</a> and <a href="https://usa.united-imaging.com/">United Imaging Intelligence (UII)</a>.
<!--I spent two wonderful summers at Siemens CT as an intern in the computer vision research group.
In the summer of 2019 I did an internship in the computer vision and robotics group at United Imaging Intelligence (UII) America where I worked on human 3D reconstruction.--> 
My <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620749.pdf">method</a> for 3D human reconstruction was integrated in a new generation of
<a href="https://global.united-imaging.com/en/product-service/products/ct/uct-960">medical CT scanners</a> capable of modeling patients in 3D and automating the scanning process. The scanners
have been deployed in more than 300 hospitals worldwide including in <a href="https://www.linkedin.com/feed/update/urn:li:activity:7024005067187888128/">Cyprus!</a>
<!--
Prior to joining George Mason University I completed my Diploma in Computer Engineering from the <a href="http://www.tuc.gr/4992.html">
Technical University of Crete</a> in Greece.
I worked with Dr. <a href="http://www.intelligence.tuc.gr/~lagoudakis/">Michail G. Lagoudakis</a> on landmark recognition and localization for the RobotStadium online robotic soccer competition.
-->
                </p>
                <p style="text-align:center">
                  <a href="mailto:georgios.georgakis@jpl.nasa.gov">Email</a> &nbsp;/&nbsp;
                  <a href="CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=ndbhEbYAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/georgios-georgakis-517a1a76/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/ggeorgak11">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:25%;max-width:37%">
                <a href="images/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Papers</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <!--
    <tr onmouseout="bolt3d_stop()" onmouseover="bolt3d_start()"  bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bolt3d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/bolt3d.mp4" type="video/mp4">
          Your browser does not support the video tag.

          </video></div>
          <img src='images/bolt3d.jpg' width="160">
        </div>
        <script type="text/javascript">
          function bolt3d_start() {
            document.getElementById('bolt3d_image').style.opacity = "1";
          }

          function bolt3d_stop() {
            document.getElementById('bolt3d_image').style.opacity = "0";
          }
          bolt3d_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://szymanowiczs.github.io/bolt3d">
          <span class="papertitle">Bolt3D: Generating 3D Scenes in Seconds</span>
        </a>
        <br>
        <a href="https://szymanowiczs.github.io/">Stanislaw Szymanowicz</a>,
        <a href="https://jasonyzhang.com">Jason Y. Zhang</a>,
        <a href="https://pratulsrinivasan.github.io">Pratul Srinivasan</a>,
        <a href="https://ruiqigao.github.io">Ruiqi Gao</a>,
        <a href="https://github.com/ArthurBrussee">Arthur Brussee</a>,
        <a href="https://holynski.org">Aleksander Holynski</a>,
        <a href="https://ricardomartinbrualla.com">Ricardo Martin-Brualla</a>,
		<strong>Jonathan T. Barron</strong>,
        <a href="https://henzler.github.io">Philipp Henzler</a>
        <br>
        <em>ICCV</em>, 2025
        <br>
        <a href="https://szymanowiczs.github.io/bolt3d">project page</a>
        /
        <a href="https://szymanowiczs.github.io/bolt3d">arXiv</a>
        <p></p>
        <p>
		By training a latent diffusion model to directly output 3D Gaussians we enable fast (~6 seconds on a single GPU) feed-forward 3D scene generation.
        </p>
      </td>
    </tr>
    -->


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/ingenuity_mbl_0.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://doi.org/10.1109/TFR.2025.3612364">
          <span class="papertitle">A Map-based Localization System for Ingenuity using Deep Image Matching</span>
        </a>
        <br>
        <strong>Georgios Georgakis</strong>, 
        <a href="https://www.linkedin.com/in/dario-pisanti-29a671172">Dario Pisanti</a>, 
        <a href="https://www.jpl.nasa.gov/site/research/nwilliam/">Nathan Williams</a>,  
        <a href="https://scholar.google.com/citations?user=u3Uzh7MAAAAJ&hl=en">Cecilia Mauceri</a>,
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/gerik_kubiak/">Gerik Kubiak</a>, 
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/adnan_ansar/">Adnan Ansar</a>, 
        <a href="https://scholar.google.com/citations?user=uwadDKYAAAAJ&hl=en">Roland Brockers</a> 
        <br>
        <em>IEEE Transactions on Field Robotics (<b>T-FR</b>) Special Issue on Space Robotics</em>, 2025
        <br>
        <a href="https://drive.google.com/file/d/1ycjCXZVaygvEQ14_4bK51vtHIrcw71Ri/view?usp=drive_link">pdf</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/vpe_main.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2508.11584">
          <span class="papertitle">Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks</span>
        </a>
        <br>
        <a href="https://ch.linkedin.com/in/jakublucki?trk=people-guest_people_search-card">Jakub Lucki</a>, 
        <a href="https://scholar.google.com/citations?user=Ih2eXN8AAAAJ&hl=en">Jonathan Becktor</a>, 
        <strong>Georgios Georgakis</strong>, 
        <a href="https://github.com/robroyce">Rob Royce</a>, 
        <a href="https://scholar.google.com/citations?user=CZ1hgVoAAAAJ&hl=en">Shehryar Khattak</a> 
        <br>
        <em>arXiv:2508.11584 (under submission)</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2508.11584">pdf</a> / <a href="https://github.com/nasa-jpl/visual-perception-engine">code</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/salient_edge_rendering.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/11177175">
          <span class="papertitle">Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity</span>
        </a>
        <br>
        <a href="https://hoa.pm/">Tu-Hoa Pham</a>, 
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/philip_bailey/">Philip Bailey</a>, 
        <a href="https://www.linkedin.com/in/dposadac">Daniel Posada</a>, 
        <strong>Georgios Georgakis</strong>, 
        <a href="https://www.linkedin.com/in/jorge-enriquez-224604236">Jorge Enriquez</a>, 
        <a href="https://www.linkedin.com/in/surya-suresh1">Surya Suresh</a>, 
        <a href="https://scholar.google.com/citations?user=DCZIe6oAAAAJ&hl=en">Marco Dolci</a>, 
        <a href="https://science.nasa.gov/people/philip-y-twu/">Philip Twu</a> 
        <br>
        <em>IEEE Robotics and Automation Letters (<b>RA-L</b>)</em>, 2025
        <br>
        <a href="https://drive.google.com/file/d/1GB0t8mxzd2GO20_4ifTDIDxVDf-P8AEp/view?usp=sharing">pdf</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/risk_guided_diffusion_title.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://openreview.net/pdf?id=iTSUgd0KNt">
          <span class="papertitle">Risk-Guided Diffusion: Toward Deploying Robot Foundation Models In Space, Where Failure Is Not An Option</span>
        </a>
        <br>
        <a href="https://www.rohanthakker.in/">Rohan Thakker</a>, 
        <a href="https://www.linkedin.com/in/adarsh-patnaik-a57667149/">Adarsh Patnaik</a>, 
        <a href="https://vincekurtz.github.io/">Vince Kurtz</a>,  
        <a href="https://jonasfrey96.github.io/">Jonas Frey</a>, 
        <a href="https://scholar.google.com/citations?user=Ih2eXN8AAAAJ&hl=en">Jonathan Becktor</a>,
        <a href="https://www.linkedin.com/in/sangwoomoon/">Sangwoo Moon</a>, 
        <a href="https://github.com/robroyce">Rob Royce</a>, 
        <a href="https://scholar.google.com/citations?user=L0YXPuwAAAAJ&hl=en">Marcel Kaufmann</a>, 
        <strong>Georgios Georgakis</strong>,  
        <a href="https://pascal-roth.github.io/">Pascal Roth</a>, 
        <a href="http://robotics.caltech.edu/wiki/index.php/JoelBurdick">Joel Burdick</a>, 
        <a href="https://scholar.google.com/citations?user=DO3quJYAAAAJ&hl=en">Marco Hutter</a>, 
        <a href="https://scholar.google.com/citations?user=CZ1hgVoAAAAJ&hl=en">Shehryar Khattak</a> 
        <br>
        <em>Workshop on Reliable Robotics: Safety and Security in the Face of Generative AI at RSS</em>, 2025 &nbsp <font color="red"><strong>(Best paper award)</strong></font>
        <br>
        <a href="https://openreview.net/pdf?id=iTSUgd0KNt">pdf</a>
      </td>
    </tr>



    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/arna_overview.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2506.17462">
          <span class="papertitle">General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting</span>
        </a>
        <br>
        <a href="https://web.stanford.edu/~blange/">Bernard Lange</a>, 
        <a href="https://scholar.google.com/citations?user=oigOlJEAAAAJ&hl=tr">Anil Yildiz</a>, 
        <a href="https://mansurarief.github.io/">Mansur Arief</a>, 
        <a href="https://scholar.google.com/citations?user=CZ1hgVoAAAAJ&hl=en">Shehryar Khattak</a>, 
        <a href="https://mykel.kochenderfer.com/">Mykel Kochenderfer</a>, 
        <strong>Georgios Georgakis</strong> 
        <br>
        <em>arXiv:2506.17462 (under submission)</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2506.17462">pdf</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/concept_large_corr.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2502.09795">
          <span class="papertitle">Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging Illumination Conditions</span>
        </a>
        <br>
        <a href="https://www.linkedin.com/in/dario-pisanti-29a671172">Dario Pisanti</a>, 
        <a href="https://scholar.google.it/citations?user=__DpwBcAAAAJ&hl=en">Robert Hewitt</a>, 
        <a href="https://scholar.google.com/citations?user=uwadDKYAAAAJ&hl=en">Roland Brockers</a>, 
        <strong>Georgios Georgakis</strong>
        <br>
        <em>arXiv:2502.09795 (under submission)</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2502.09795">pdf</a> / <a href="https://github.com/nasa-jpl/mbl_mars">code-MbL</a> / <a href="https://github.com/nasa-jpl/martian">code-MARTIAN</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/jaxa.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2510.11817">
          <span class="papertitle">Enhancing the Quality of 3D Lunar Maps Using JAXA’s Kaguya Imagery</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=YYPwRboAAAAJ&hl=en">Yumi Iwashita</a>, 
        <a href="https://no.linkedin.com/in/h%C3%A5kon-moe-582b38232">Haakon Moe</a>,
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/yang_cheng/">Yang Cheng</a>, 
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/adnan_ansar/">Adnan Ansar</a>, 
        <strong>Georgios Georgakis</strong>, 
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/adrian_stoica/">Adrian Stoica</a>, 
        <a href="https://kazuto1011.github.io/">Kazuto Nakashima</a>, 
        <a href="https://robotics.ait.kyushu-u.ac.jp/kurazume/en/">Ryo Kurazume</a>, 
        <a href="https://www.jimtoer.no/">Jim Torresen</a>  
        <br>
        <em>IEEE International Conference on Systems, Man, and Cybernetics (<b>SMC</b>)</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2510.11817">pdf</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/figure3_canvas.jpg" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arc.aiaa.org/doi/10.2514/6.2025-2073">
          <span class="papertitle">Illumination Invariant Image Matching for Lunar TRN</span>
        </a>
        <br>
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/noah-rothenberger/">Noah Rothenberger</a>, 
        <strong>Georgios Georgakis</strong>,  
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/yang_cheng/">Yang Chen</a>, 
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/adnan_ansar/">Adnan Ansar</a> 
        <br>
        <em>American Institute of Aeronautics and Astronautics (AIAA) SciTech</em>, 2025
        <br>
        <a href="https://drive.google.com/file/d/10O-6Q9ujkxPZ8bVEY5JarR7Hg7t2dSRb/view?usp=sharing">pdf</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/clip_sample.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2412.13026">
          <span class="papertitle">NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for Vision and Language Navigation</span>
        </a>
        <br>
        <a href="https://wanchoo93.github.io/">Karan Wanchoo</a>, 
        <a href="https://www.linkedin.com/in/xiaoye-zuo-532106186">Xiaoye Zuo</a>, 
        <a href="https://isgla.github.io/">Hannah Gonzalez</a>, 
        <a href="https://sdan2.github.io/">Soham Dan</a>, 
        <strong>Georgios Georgakis</strong>,  
        <a href="https://www.cis.upenn.edu/~danroth/">Dan Roth</a>, 
        <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>, 
        <a href="https://www.miltsakaki.com/">Eleni Miltsakaki</a> 
        <br>
        <em>arXiv:2412.13026</em>, 2024
        <br>
        <a href="https://arxiv.org/pdf/2412.13026">pdf</a>
      </td>
    </tr>
    
    
    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/attn_viz2.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://seal.ae.gatech.edu/sites/default/files/2024-09/GeorgakisGeorgios.pdf">
          <span class="papertitle">Learning Illumination Invariant Features for Lunar South Pole with Deep Learning</span>
        </a>
        <br>
        <strong>Georgios Georgakis</strong>,  
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/adnan_ansar/">Adnan Ansar</a>
        <br>
        <em>Space Imaging Workshop (<b>SIW</b>)</em>, 2024
        <br>
        <a href="https://seal.ae.gatech.edu/sites/default/files/2024-09/GeorgakisGeorgios.pdf">pdf</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/pixel2elev.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/10517395">
          <span class="papertitle">Pixel to Elevation: Learning to Predict Elevation Maps at Long Range using Images for Autonomous Offroad Navigation</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=osVxrIEAAAAJ&hl=ko">Chanyoung Chung</a>, 
        <strong>Georgios Georgakis</strong>,  
        <a href="https://scholar.google.com/citations?user=s6dAIlEAAAAJ&hl=en">Patrick Spieler</a>, 
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/curtis_padgett/">Curtis Padgett</a>, 
        <a href="https://scholar.google.com/citations?user=36qnUD4AAAAJ&hl=en">Ali Agha</a>, 
        <a href="https://scholar.google.com/citations?user=CZ1hgVoAAAAJ&hl=en">Shehryar Khattak</a> 
        <br>
        <em>IEEE Robotics and Automation Letters (<b>RA-L</b>)</em>, 2024
        <br>
        <a href="https://drive.google.com/file/d/1-fQDqM46ghoYCmuUjOBHba_dRR2hoURj/view?usp=sharing">pdf</a> / <a href="https://arxiv.org/pdf/2401.17484.pdf">arXiv</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/aeroconf2024.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/10521439">
          <span class="papertitle">Icy Moon Surface Simulation and Stereo Depth Estimation for Sampling Autonomy</span>
        </a>
        <br>
        <a href="https://ram-bhaskara.github.io/">Ramchander Bhaskara</a>, 
        <strong>Georgios Georgakis</strong>,  
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/jeremy_nash/">Jeremy Nash</a>, 
        <a href="https://scholar.google.com/citations?user=rYoPYlcAAAAJ&hl=en">Marissa Cameron</a>,
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/joseph_bowkett/">Joseph Bowkett</a>, 
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/adnan_ansar/">Adnan Ansar</a>, 
        <a href="https://scholar.google.com/citations?user=paVge2QAAAAJ">Manoranjan Majji</a>,
        <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/paul_backes/">Paul Backes</a> 
        <br>
        <em>IEEE Aerospace Conference (<b>AeroConf</b>)</em>, 2024
        <br>
        <a href="https://drive.google.com/file/d/1og3l-dBrstetr0PTYSbeng4YKhscus7W/view?usp=sharing">pdf</a> / <a href="https://arxiv.org/pdf/2401.12414.pdf">arXiv</a> / <a href="https://github.com/nasa-jpl/guiss">code</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/multi-object_nav.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://embodied-ai.org/papers/2023/19.pdf">
          <span class="papertitle">Unordered Navigation to Multiple Semantic Targets in Novel Environments</span>
        </a>
        <br>
        <a href="https://bucherb.github.io/">Bernadette Bucher</a>*, 
        <a href="https://www.linkedin.com/in/katrina-ashton/?originalSubdomain=au">Katrina Ashton</a>*, 
        <a href="https://scholar.google.com/citations?user=N2Le_7cAAAAJ&hl=zh-CN">Bo Wu</a>, 
        <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>, 
        <a href="https://laughbuddha.github.io/">Siddharth Goel</a>, 
        <a href="https://nikolaimatni.github.io/">Nikolai Matni</a>, 
        <strong>Georgios Georgakis</strong>,  
        <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> 
        <br>
        <em>Embodied AI Workshop at CVPR</em>, 2023
        <br>
        * equal contribution <br>
        <a href="https://embodied-ai.org/papers/2023/19.pdf">pdf</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/bridgeData_title.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://www.roboticsproceedings.org/rss18/p063.pdf">
          <span class="papertitle">Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets</span>
        </a>
        <br>
        <a href="https://febert.github.io/">Frederick Ebert</a>*, 
        <a href="https://www.linkedin.com/in/yanlai-yang/">Yanlai Yang</a>*, 
        <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>, 
        <a href="https://bucherb.github.io/">Bernadette Bucher</a>, 
        <strong>Georgios Georgakis</strong>, 
        <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>, 
        <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, 
        <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> 
        <br>
        <em>Robotics: Science and Systems (<b>RSS</b>)</em>, 2022
        <br>
        * equal contribution <br>
        <a href="https://sites.google.com/view/bridgedata">project page</a> / <a href="https://www.roboticsproceedings.org/rss18/p063.pdf">pdf</a> / <a href="https://arxiv.org/pdf/2109.13396.pdf">arXiv</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/vln_title.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Georgakis_Cross-Modal_Map_Learning_for_Vision_and_Language_Navigation_CVPR_2022_paper.pdf">
          <span class="papertitle">Cross-modal Map Learning for Vision and Language Navigation</span>
        </a>
        <br>
        <strong>Georgios Georgakis</strong>, 
        <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>, 
        <a href="https://wanchoo93.github.io/">Karan Wanchoo</a>, 
        <a href="https://sdan2.github.io/">Soham Dan</a>,
        <a href="https://www.miltsakaki.com/">Eleni Miltsakaki</a>, 
        <a href="https://www.cis.upenn.edu/~danroth/">Dan Roth</a>, 
        <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> 
        <br>
        <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
        <br>
        <a href="https://ggeorgak11.github.io/CM2-project/">project page</a> / 
        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Georgakis_Cross-Modal_Map_Learning_for_Vision_and_Language_Navigation_CVPR_2022_paper.pdf">pdf</a> / 
        <a href="https://arxiv.org/pdf/2203.05137.pdf">arXiv</a> / 
        <a href="https://github.com/ggeorgak11/CM2">code</a> / 
        <a href="https://www.youtube.com/watch?v=G4ILHh7KrFc">talk video</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/nav_policy2.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://par.nsf.gov/servlets/purl/10340845">
          <span class="papertitle">Learning to Map for Active Semantic Goal Navigation</span>
        </a>
        <br>
        <strong>Georgios Georgakis</strong>*, 
        <a href="https://bucherb.github.io/">Bernadette Bucher</a>*, 
        <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>,
        <a href="https://singh-sid930.github.io/"> Siddharth Singh </a>, 
        <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> 
        <br>
        <em>International Conference on Learning Representations (<b>ICLR</b>)</em>, 2022
        <br>
        * equal contribution <br>
        <a href="https://ggeorgak11.github.io/uncertainty-nav-project/">project page</a> / 
        <a href="https://par.nsf.gov/servlets/purl/10340845">pdf</a> / 
        <a href="https://arxiv.org/pdf/2106.15648.pdf">arXiv</a> / 
        <a href="https://github.com/ggeorgak11/L2M">code</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/UPEN_system_policy.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/9812423">
          <span class="papertitle">Uncertainty-driven Planner for Exploration and Navigation</span>
        </a>
        <br>
        <strong>Georgios Georgakis</strong>, 
        <a href="https://bucherb.github.io/">Bernadette Bucher</a>, 
        <a href="https://www.linkedin.com/in/anton-arapin-9494b616b">Anton Arapin</a>, 
        <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>,
        <a href="https://nikolaimatni.github.io/">Nikolai Matni</a>, 
        <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> 
        <br>
        <em>IEEE Conference on Robotics and Automation (<b>ICRA</b>)</em>, 2022
        <br>
        <a href="https://ggeorgak11.github.io/uncertainty-nav-project/">project page</a> / 
        <a href="https://drive.google.com/file/d/1B2WcISDfs27P0Qw_ea25vhn-AR88ZZCS/view?usp=sharing">pdf</a> / 
        <a href="https://arxiv.org/pdf/2202.11907.pdf">arXiv</a> / 
        <a href="https://github.com/ggeorgak11/UPEN">code</a> /
        <a href="https://www.youtube.com/watch?v=_j6NA4Rg7xU">Demo @ICRA</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/opa_title.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561541">
          <span class="papertitle">Object-centric Video Prediction without Annotation</span>
        </a>
        <br>
        <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>*, 
        <strong>Georgios Georgakis</strong>*, 
        <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> 
        <br>
        <em>IEEE Conference on Robotics and Automation (<b>ICRA</b>)</em>, 2021
        <br>
        * equal contribution <br>
        <a href="https://drive.google.com/file/d/1XNujyeh-2d4VMO1E08oSithnZJNNpAvd/view?usp=sharing">pdf</a> / 
        <a href="https://arxiv.org/pdf/2105.02799.pdf">arXiv</a> / 
        <a href="https://github.com/kschmeckpeper/opa">code</a> /
        <a href="https://www.youtube.com/watch?v=MFdz-dwSaLg">slides video</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/HKMR_title.PNG" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620749.pdf">
          <span class="papertitle">Hierarchical Kinematic Human Mesh Recovery</span>
        </a>
        <br>
        <strong>Georgios Georgakis</strong>*, 
        <a href="https://liren2515.github.io/page/">Ren Li</a>*, 
        <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
        <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, 
        <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a>,
        <a href="http://wuziyan.com/">Ziyan Wu</a> 
        <br>
        <em>European Conference on Computer Vision (<b>ECCV</b>)</em>, 2020
        <br>
        * equal contribution <br>
        <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620749.pdf">pdf</a> / 
        <a href="https://drive.google.com/file/d/1LHtdMyeGDIgeI9x0RFQNdrbFr6YlGt0Q/view?usp=sharing">supplementary</a> /
        <a href="https://arxiv.org/pdf/2003.04232">arXiv</a> /
        <a href="https://drive.google.com/file/d/1wEqCMjBSPMZODoCQMFw_KoQd78yY7HMq/view?usp=sharing">long slides</a> /
        <a href="https://drive.google.com/file/d/1B0pXIIt5HEeV_tVJXS4iTYg2peR-zWKU/view?usp=sharing">short slides</a> /
        <a href="https://drive.google.com/file/d/16ZatXrWQXQDX5KBrvQSWUSM6uStFD-5F/view?usp=sharing">long video</a> /
        <a href="https://drive.google.com/file/d/1xco36-d1na1gcwyphC2zXmwCXALA3aER/view?usp=sharing">short video</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/miccai_2020.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620749.pdf">
          <span class="papertitle">Robust Multi-modal 3D Patient Body Modeling</span>
        </a>
        <br>
        <a href="https://fyangneil.github.io/">Fan Yang</a>, 
        <a href="https://liren2515.github.io/page/">Ren Li</a>, 
        <strong>Georgios Georgakis</strong>,  
        <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
        <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, 
        <a href="https://haibinling.github.io/">Haibin Ling</a>, 
        <a href="http://wuziyan.com/">Ziyan Wu</a> 
        <br>
        <em>Medical Image Computing and Computer Assisted Interventions (<b>MICCAI</b>)</em>, 2020
        <br>
        * equal contribution <br>
        <a href="https://www.researchgate.net/profile/Fan-Yang-67/publication/345262231_Robust_Multi-modal_3D_Patient_Body_Modeling/links/5fba3909a6fdcc6cc65a6677/Robust-Multi-modal-3D-Patient-Body-Modeling.pdf">pdf</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/title_nav.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/1911.07980.pdf">
          <span class="papertitle">Simultaneous Mapping and Target Driven Navigation</span>
        </a>
        <br>
        <strong>Georgios Georgakis</strong>, 
        <a href="https://yimengli46.github.io/">Yimeng Li</a>,
        <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> 
        <br>
        <em>arXiv:1911.07980</em>, 2020
        <br>
        <a href="https://arxiv.org/pdf/1911.07980.pdf">pdf</a> / 
        <a href="https://github.com/ggeorgak11/mapping_navigation">code</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/human_pose_cai.PNG" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/1911.07383v1.pdf">
          <span class="papertitle">Towards Robust RGB-D Human Mesh Recovery</span>
        </a>
        <br>
        <a href="https://liren2515.github.io/page/">Ren Li</a>, 
        <a href="https://www.changjiangcai.com/">Changjiang Cai</a>, 
        <strong>Georgios Georgakis</strong>,  
        <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
        <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, 
        <a href="http://wuziyan.com/">Ziyan Wu</a> 
        <br>
        <em>arXiv:1911.07383</em>, 2019
        <br>
        <a href="https://arxiv.org/pdf/1911.07383v1.pdf">pdf</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/title_2019.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Georgakis_Learning_Local_RGB-to-CAD_Correspondences_for_Object_Pose_Estimation_ICCV_2019_paper.pdf">
          <span class="papertitle">Learning Local RGB-to-CAD Correspondences for Object Pose Estimation</span>
        </a>
        <br>
        <strong>Georgios Georgakis</strong>, 
        <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
        <a href="http://wuziyan.com/">Ziyan Wu</a>,
        <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> 
        <br>
        <em>IEEE International Conference on Computer Vision (<b>ICCV</b>)</em>, 2019
        <br>
        <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Georgakis_Learning_Local_RGB-to-CAD_Correspondences_for_Object_Pose_Estimation_ICCV_2019_paper.pdf">pdf</a> /
        <a href="https://drive.google.com/file/d/1su-0PtW1rMCK2yNCyIfW3smoFPdUijOu/view?usp=sharing">slides</a> /
        <a href="https://drive.google.com/file/d/1fRQsIRsluVTZQ2TNC98BYvWpIiR4HfSQ/view?usp=sharing">poster</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/title_cvpr18.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Georgakis_End-to-End_Learning_of_CVPR_2018_paper.pdf">
          <span class="papertitle">End-to-end Learning for Keypoint Detection and Descriptor for Pose Invariant 3D Matching</span>
        </a>
        <br>
	      <strong>Georgios Georgakis</strong>, 
        <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
        <a href="http://wuziyan.com/">Ziyan Wu</a>, 
        <a href="https://scholar.google.com/citations?user=hFUJkFgAAAAJ&hl=en">Jan Ernst</a>, 
        <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> 
        <br>
        <em>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2018
        <br>
        <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Georgakis_End-to-End_Learning_of_CVPR_2018_paper.pdf">pdf</a> /
        <a href="https://drive.google.com/file/d/1cZzbsUDP7y4hww0BmHrT1ZKX2r2juhFj/view?usp=sharing">supplementary</a> /
        <a href="https://drive.google.com/file/d/1cMBSdpYL_LnuvmZPlcUe_gorY3vuGEX6/view?usp=sharing">poster</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/iros17_title.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/8206371">
          <span class="papertitle">Label Propagation in RGB-D Video</span>
        </a>
        <br>
        <a href="https://cs.gmu.edu/~mreza/">Md Alimoor Reza</a>, 
        <a href="https://scholar.google.com/citations?user=5DQ7jkwAAAAJ&hl=en">Hui Zheng</a>, 
        <strong>Georgios Georgakis</strong>, 
        <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> 
        <br>
        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>)</em>, 2017
        <br>
        <a href="https://drive.google.com/file/d/1eJQ99hGSgSPl5o6iK3Lk4U8qLflYjuAW/view?usp=sharing">pdf</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/synth_title.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="http://roboticsproceedings.org/rss13/p43.pdf">
          <span class="papertitle">Synthesizing Training Data for Object Detection in Indoor Scenes</span>
        </a>
        <br>
        <strong>Georgios Georgakis</strong>, 
        <a href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a>, 
        <a href="http://acberg.com/">Alexander C. Berg</a>,
        <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> 
        <br>
        <em>Robotics: Science and Systems (<b>RSS</b>)</em>, 2017
        <br>
        <a href="http://cs.gmu.edu/~robot/synthesizing.html">project page</a> /
        <a href="http://roboticsproceedings.org/rss13/p43.pdf">pdf</a> /
        <a href="https://drive.google.com/file/d/1XSE3jEVc3mddpIjc6Jmc3o9LmrGKRS0P/view?usp=sharing">poster</a> /
        <a href="https://drive.google.com/file/d/1OZnuQVBX7o4I4byRX1P6RZPOrQCWxXpj/view?usp=sharing">slides</a> /
        <a href="https://drive.google.com/file/d/1dXBUwFymmI3mSDlu58DktldWigE4VLZ8/view?usp=sharing">video</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/contact_title.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="">
          <span class="papertitle">A Contact Exploitative Approach to the Amazon Robotics Challenge</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=3kjVPtcAAAAJ&hl=en">Eadom Dessalene</a>, 
        <strong>Georgios Georgakis</strong>, 
        <a href="https://cs.gmu.edu/~mreza/">Md Alimoor Reza</a>, 
        <a href="https://yimengli46.github.io/">Yimeng Li</a>,
        <a href="https://www.linkedin.com/in/yossi-ovcharik/?originalSubdomain=il">Yossi Ovcharik</a>, 
        <a href="https://scholar.google.com/citations?user=Cb7dj-UAAAAJ&hl=en">Amir Shapiro</a>, 
        <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a>,
        <a>Daniel Lofaro</a> 
        <br>
        <em>Warehouse Picking Automation Workshop at ICRA</em>, 2017
        <br>
        <a href="https://drive.google.com/file/d/1Lg1sPeZbK3D5jH_vOm3mbUikzLB0zJgg/view?usp=sharing">pdf</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/title_dataset.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/7785117">
          <span class="papertitle">Multiview RGB-D Dataset for Object Instance Detection</span>
        </a>
        <br>
        <strong>Georgios Georgakis</strong>, 
        <a href="https://cs.gmu.edu/~mreza/">Md Alimoor Reza</a>, 
        <a href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a>, 
        <a href="https://scholar.google.com/citations?user=2-meErUAAAAJ&hl=en">Le Phi-Hung</a>,
        <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> 
        <br>
        <em>International Conference on 3D Vision (<b>3DV</b>)</em>, 2016
        <br>
        <a href="http://cs.gmu.edu/~robot/gmu-kitchens.html">project page</a> /
        <a href="https://drive.google.com/file/d/1cV9nVnEnN1nUMN3PZGRlnMdUc5Xs8oLA/view?usp=sharing">pdf</a> /
        <a href="https://arxiv.org/pdf/1609.07826.pdf">arXiv</a> /
        <a href="https://drive.google.com/file/d/1k0OJTt_i92Zk_p_Qw0MHvX-wsH7xmjsu/view?usp=sharing">poster</a>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src="images/iros16_title.png" width="250" style="border-style: none">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/7759607">
          <span class="papertitle">RGB-D Multiview Object Detection with Object Proposals and Shape Context</span>
        </a>
        <br>
        <strong>Georgios Georgakis</strong>, 
        <a href="https://cs.gmu.edu/~mreza/">Md Alimoor Reza</a>,
        <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> 
        <br>
        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>)</em>, 2016
        <br>
        <a href="https://drive.google.com/file/d/1vGUZAJtrfgmEmOdkOnmn0oItCNgBM_yu/view?usp=sharing">pdf</a> /
        <a href="https://drive.google.com/file/d/1_EEPHQ_WCcSIdvI6eTX_J-sESb3nshi5/view?usp=sharing">poster</a> /
        <a href="https://drive.google.com/file/d/1vBNml6hScDR3MgZZcrTCuiYJn9XRvOc_/view?usp=sharing">short slides</a> / 
        <a href="https://drive.google.com/file/d/1MhRhLmgvy0rXn0z-LAWhDXs4aTElRZGC/view?usp=sharing">code</a>
      </td>
    </tr>


    </tbody></table>

          


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
              <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                
                <ul>

                <li>
                Instructor, University of Pennsylvania
                <br>
                ESE 650 Learning in Robotics, Spring 2022
                <br>
                <a href="https://sites.google.com/seas.upenn.edu/ese650spr2022">Website</a> /
                <a href="21_ese650.pdf">Notes: Spring 2021</a> from <a href="https://pratikac.github.io/">Pratik Chaudhari</a>
                </li>
                <li>
                Graduate Teaching Assistant, George Mason University
                <br>
                CS112: Introduction to Programming, 2014-2017 
                <br>
                CS480: Introduction to Artificial Intelligence, 2019
                </li>

                </ul>
              </td>
            </tr>
           

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Awards</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                
                <ul>

                <li> <a>Team Award</a>, “For Lunar Navigation Maps (LuNaMaps) project to address key gaps in mapping for Lunar TRN” <br>
                    Robotic Systems Section, Jet Propulsion Lab, September 2025
                </li>
                <li> <a>Team Award</a>, “For technical contributions and leadership in advancing robotic autonomy” <br>
                    Robotic Systems Section, Jet Propulsion Lab, September 2025
                </li>
                <li> <a>Technology Spotlight Award</a>, “Map-based Localization for Ingenuity flights” <br>
                    Autonomous Systems Division, Jet Propulsion Lab, August 2025
                </li>
                <li> <a>Technology Spotlight Award</a>, “Photometric Calibration of JunoCam with a Data-driven Method” <br>
                    Autonomous Systems Division, Jet Propulsion Lab, May 2024
                </li>

                </ul>
                
                <br>
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Selected Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                
                <ul>

                <li>
                <a href="https://drive.google.com/file/d/1M09QayWNaV42i-5D0dUSx26P8lSRKPEQ/view?usp=sharing">Approximate Photometric Calibrations of JunoCam with a Deep Learning System</a>
                <br>
                Invited talk at Juno Science Team Meeting, September 2025				  
                </li>
                <li>
                <a href="https://www.youtube.com/watch?v=G4ILHh7KrFc">Cross-modal Map Learning for Vision and Language Navigation</a>
                <br>
                GRASP SFI Seminar Series, Philadelphia PA, April 2022
                </li>

                </ul>

              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Selected Patents</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
              
              <ul>

              <li>
              <i>Method and System for On-board Localization</i>,
              <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/roland_brockers/">Roland Brockers</a>, 
              <a href="https://scholar.google.co.za/citations?user=f27ESCMAAAAJ&hl=ja">Friedrich Dietsche</a>, 
              <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/jeff_delaune/">Jeff Delaune</a>, 
              <a href="https://pedropro.github.io/">Pedro Proença</a>, 
              <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/robert_hewitt/">Robert Hewitt</a>, 
              <strong>Georgios Georgakis</strong> 
              <br>
              US18/312,444 filed May 2023, Application no. <a href="https://patents.google.com/patent/US20230360547A1/en">US20230360547A1</a>
              </li>

              <li>
              <i>Systems and Methods for Human Pose and Mesh Recovery</i>,
              <a href="http://wuziyan.com/">Ziyan Wu</a>, 
              <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
              <a href="https://www.changjiangcai.com/">Changjiang Cai</a>, 
              <strong>Georgios Georgakis</strong> 
              <br>
              US16/995,446, filed August 2020, Application no. <a href="https://patents.google.com/patent/US20210158028A1/en?oq=16995446">US20210158028A1</a>
              </li>

              <li>
              <i>Learning Keypoints and Matching RGB Images to CAD Models</i>,
              <strong>Georgios Georgakis</strong>, 
              <a href="https://karanams.github.io/">Srikrishna Karanam</a>, 
              <a href="http://wuziyan.com/">Ziyan Wu</a>, 
              <a href="https://scholar.google.com/citations?user=hFUJkFgAAAAJ&hl=en">Jan Ernst</a> 
              <br>
              PCT/US2019/053827, filed September 2019, Application no. <a href="https://patents.google.com/patent/WO2020086217A1/en?oq=WO2020086217">WO2020086217</a>
              </li>

              </ul>

              </td>
            </tr>
						
						
            
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>
                </p>
              </td>
            </tr>
          </tbody></table>


        </td>
      </tr>
    </table>
  </body>
</html>
