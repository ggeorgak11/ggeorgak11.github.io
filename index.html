<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>Georgios Georgakis</title>
<link rel="stylesheet" type="text/css" href="style.css">
</head>


<img src="images/profile.jpg" style="width: 15%;">

<h2>Georgios Georgakis</h2>
I am a Robotics Technologist at the <a href="https://www.jpl.nasa.gov/">NASA-Jet Propulsion Laboratory</a>. 
My research interests lie at the intersection of computer vision and machine learning, with applications in robotics.
My long-term goal is enabling the exploration and understanding of our solar system by developing generalizable, 
learning-based perception and navigation algorithms for robotic systems,
and by designing AI models that maximize the scientific return from space mission data.
<br><br>
<!-- At JPL, I have been developing perception algorithms in support of multiple autonomous systems, including a future Mars Science Helicopter. -->
Prior to joining JPL, I was a Postdoctoral Researcher in the <a href="https://www.grasp.upenn.edu/">GRASP Lab</a> at the University of Pennsylvania, advised 
by <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>. During my PostDoc I worked on vision-based semantic navigation in novel environments
for the tasks of exploration, object-goal navigation, and vision and language navigation (VLN).
I received my Ph.D from the Department of <a href="http://cs.gmu.edu/">Computer Science</a> 
at <a href="https://www2.gmu.edu/">George Mason University</a>, advised by <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a>.
I completed my Diploma in Computer Engineering from the <a href="http://www.tuc.gr/4992.html">
Technical University of Crete</a> in Greece, where I worked with Dr. <a href="http://www.intelligence.tuc.gr/~lagoudakis/">Michail G. Lagoudakis</a> on 
landmark recognition and localization for robotic soccer.
<!-- During my Ph.D I worked on multiple problems such as object detection and recognition
in RGB-D data, keypoint and descriptor learning, and 3D object pose estimation.--> 
<br><br>
I spent time as a research intern at <a href="https://www.siemens.com/us/en.html">Siemens</a> and <a href="https://usa.united-imaging.com/">United Imaging Intelligence (UII)</a>.
<!--I spent two wonderful summers at Siemens CT as an intern in the computer vision research group.
In the summer of 2019 I did an internship in the computer vision and robotics group at United Imaging Intelligence (UII) America where I worked on human 3D reconstruction.--> 
My <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620749.pdf">method</a> for 3D human reconstruction was integrated in a new generation of
<a href="https://global.united-imaging.com/en/product-service/products/ct/uct-960">medical CT scanners</a> capable of modeling patients in 3D and automating the scanning process. The scanners
have been deployed in more than 300 hospitals worldwide including in <a href="https://www.linkedin.com/feed/update/urn:li:activity:7024005067187888128/">Cyprus!</a>
<!--
Prior to joining George Mason University I completed my Diploma in Computer Engineering from the <a href="http://www.tuc.gr/4992.html">
Technical University of Crete</a> in Greece.
I worked with Dr. <a href="http://www.intelligence.tuc.gr/~lagoudakis/">Michail G. Lagoudakis</a> on landmark recognition and localization for the RobotStadium online robotic soccer competition.
-->
<br>
<br>
<a href="mailto:georgios.georgakis@jpl.nasa.gov">Email</a> | <a href="CV.pdf">CV</a> | <a href="https://scholar.google.com/citations?user=ndbhEbYAAAAJ&hl=en">Scholar</a> | <a href="https://www.linkedin.com/in/georgios-georgakis-517a1a76/">LinkedIn</a> |
 <a href="https://github.com/ggeorgak11">Github</a> 

<br>
<br>

<!--
<h4>Contact Information</h4>
198-120, 4800 Oak Grove Dr, Pasadena, CA 91109 <br>
georgios DOT georgakis AT jpl DOT nasa DOT gov
<br><br><br>
-->

<h2>Teaching</h2>
ESE 650 Learning in Robotics (Spring 2022) <br>
<a href="https://docs.google.com/document/d/1jkZ_nZ7be6kpYQsy9XXX0dTQZvQ7mQwEWEdpA0hXwmo/edit?usp=sharing">Syllabus</a> | <a href="https://sites.google.com/seas.upenn.edu/ese650spr2022">Website</a> | <a href="21_ese650.pdf">Notes: Spring 2021</a> from <a href="https://pratikac.github.io/">Pratik Chaudhari</a>

<br><br><br>

<h2>Papers</h2>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/ingenuity_mbl_0.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>A Map-based Localization System for Ingenuity using Deep Image Matching</i> <br>
    <b>Georgios Georgakis</b>, <a href="https://www.linkedin.com/in/dario-pisanti-29a671172">Dario Pisanti</a>, <a href="https://www.jpl.nasa.gov/site/research/nwilliam/">Nathan Williams</a>,  <a href="https://scholar.google.com/citations?user=u3Uzh7MAAAAJ&hl=en">Cecilia Mauceri</a>, <br>
    <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/gerik_kubiak/">Gerik Kubiak</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/adnan_ansar/">Adnan Ansar</a>, <a href="https://scholar.google.com/citations?user=uwadDKYAAAAJ&hl=en">Roland Brockers</a> <br>
	<a>IEEE Transactions on Field Robotics (<b>T-FR</b>) Special Issue on Space Robotics, 2025</a> <br>
	 [<a href="https://doi.org/10.1109/TFR.2025.3612364">pdf</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/vpe_main.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks</i> <br>
    <a href="https://ch.linkedin.com/in/jakublucki?trk=people-guest_people_search-card">Jakub Lucki</a>, <a href="https://scholar.google.com/citations?user=Ih2eXN8AAAAJ&hl=en">Jonathan Becktor</a>, <b>Georgios Georgakis</b>, <a href="https://github.com/robroyce">Rob Royce</a>, <a href="https://scholar.google.com/citations?user=CZ1hgVoAAAAJ&hl=en">Shehryar Khattak</a> <br>
	<a>arXiv:2508.11584 (under submission)</a> <br>
	 [<a href="https://arxiv.org/pdf/2508.11584">pdf</a> | <a href="https://github.com/nasa-jpl/visual-perception-engine">code</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/salient_edge_rendering.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Robust Visual Localization in Compute-Constrained Environments by Salient Edge Rendering and Weighted Hamming Similarity</i> <br>
    <a href="https://hoa.pm/">Tu-Hoa Pham</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/philip_bailey/">Philip Bailey</a>, <a href="https://www.linkedin.com/in/dposadac">Daniel Posada</a>, <b>Georgios Georgakis</b>, <br> 
    <a href="https://www.linkedin.com/in/jorge-enriquez-224604236">Jorge Enriquez</a>, <a href="https://www.linkedin.com/in/surya-suresh1">Surya Suresh</a>, <a href="https://scholar.google.com/citations?user=DCZIe6oAAAAJ&hl=en">Marco Dolci</a>, <a href="https://science.nasa.gov/people/philip-y-twu/">Philip Twu</a> <br>
	<a>IEEE Robotics and Automation Letters (<b>RA-L</b>), 2025</a> <br>
	 [<a href="https://ieeexplore.ieee.org/document/11177175">pdf</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>



<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/risk_guided_diffusion_title.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Risk-Guided Diffusion: Toward Deploying Robot Foundation Models In Space, <br> Where Failure Is Not An Option</i> <br>
    <a href="https://www.rohanthakker.in/">Rohan Thakker</a>, <a href="https://www.linkedin.com/in/adarsh-patnaik-a57667149/">Adarsh Patnaik</a>, <a href="https://vincekurtz.github.io/">Vince Kurtz</a>,  <a href="https://jonasfrey96.github.io/">Jonas Frey</a>, <a href="https://scholar.google.com/citations?user=Ih2eXN8AAAAJ&hl=en">Jonathan Becktor</a>, <br> 
    <a href="https://www.linkedin.com/in/sangwoomoon/">Sangwoo Moon</a>, <a href="https://github.com/robroyce">Rob Royce</a>, <a href="https://scholar.google.com/citations?user=L0YXPuwAAAAJ&hl=en">Marcel Kaufmann</a>, <b>Georgios Georgakis</b>, <a href="https://pascal-roth.github.io/">Pascal Roth</a>, <br> 
    <a href="http://robotics.caltech.edu/wiki/index.php/JoelBurdick">Joel Burdick</a>, <a href="https://scholar.google.com/citations?user=DO3quJYAAAAJ&hl=en">Marco Hutter</a>, <a href="https://scholar.google.com/citations?user=CZ1hgVoAAAAJ&hl=en">Shehryar Khattak</a> <br>
	<a>RSS 2025 Workshop on Reliable Robotics: Safety and Security in the Face of Generative AI</a> <br>
	 [<a href="https://openreview.net/pdf?id=iTSUgd0KNt">pdf</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>



<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/arna_overview.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting</i> <br>
    <a href="https://web.stanford.edu/~blange/">Bernard Lange</a>, <a href="https://scholar.google.com/citations?user=oigOlJEAAAAJ&hl=tr">Anil Yildiz</a>, <a href="https://mansurarief.github.io/">Mansur Arief</a>, <a href="https://scholar.google.com/citations?user=CZ1hgVoAAAAJ&hl=en">Shehryar Khattak</a>, <br> 
    <a href="https://mykel.kochenderfer.com/">Mykel Kochenderfer</a>, <b>Georgios Georgakis</b> <br>
	<a>arXiv:2506.17462 (under submission)</a> <br>
	 [<a href="https://arxiv.org/pdf/2506.17462">pdf</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/concept_large_corr.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging Illumination Conditions</i> <br>
    <a href="https://www.linkedin.com/in/dario-pisanti-29a671172">Dario Pisanti</a>, <a href="https://scholar.google.it/citations?user=__DpwBcAAAAJ&hl=en">Robert Hewitt</a>, <a href="https://scholar.google.com/citations?user=uwadDKYAAAAJ&hl=en">Roland Brockers</a>, <b>Georgios Georgakis</b> <br>
	<a>arXiv:2502.09795 (under submission)</a> <br>
	 [<a href="https://arxiv.org/pdf/2502.09795">pdf</a> | <a href="https://github.com/nasa-jpl/mbl_mars">code-MbL</a> | <a href="https://github.com/nasa-jpl/martian">code-MARTIAN</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/jaxa.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Enhancing the Quality of 3D Lunar Maps Using JAXA’s Kaguya Imagery</i> <br>
    <a href="https://scholar.google.com/citations?user=YYPwRboAAAAJ&hl=en">Yumi Iwashita</a>, <a href="https://no.linkedin.com/in/h%C3%A5kon-moe-582b38232">Haakon Moe</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/yang_cheng/">Yang Cheng</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/adnan_ansar/">Adnan Ansar</a>, <b>Georgios Georgakis</b>, <br> 
    <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/adrian_stoica/">Adrian Stoica</a>, <a href="https://kazuto1011.github.io/">Kazuto Nakashima</a>, <a href="https://robotics.ait.kyushu-u.ac.jp/kurazume/en/">Ryo Kurazume</a>, <a href="https://www.jimtoer.no/">Jim Torresen</a>  <br>
	<a>IEEE International Conference on Systems, Man, and Cybernetics (<b>SMC</b>) </a> <br>
	 [<a href="https://arxiv.org/pdf/2510.11817">pdf</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/figure3_canvas.jpg" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Illumination Invariant Image Matching for Lunar TRN</i> <br>
    <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/noah-rothenberger/">Noah Rothenberger</a>, <b>Georgios Georgakis</b>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/yang_cheng/">Yang Chen</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/adnan_ansar/">Adnan Ansar</a> <br>
	<a>AIAA SciTech, 2025</a> <br>
	 [<a href="https://arc.aiaa.org/doi/10.2514/6.2025-2073">pdf</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/clip_sample.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for Vision and Language Navigation</i> <br>
      <a href="https://wanchoo93.github.io/">Karan Wanchoo</a>, <a href="https://www.linkedin.com/in/xiaoye-zuo-532106186">Xiaoye Zuo</a>, <a href="https://isgla.github.io/">Hannah Gonzalez</a>, <a href="https://sdan2.github.io/">Soham Dan</a>, <b>Georgios Georgakis</b>, <br>
      <a href="https://www.cis.upenn.edu/~danroth/">Dan Roth</a>, <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>, <a href="https://www.miltsakaki.com/">Eleni Miltsakaki</a> <br>
	<a>arXiv:2412.13026</a> <br>
	 [<a href="https://arxiv.org/pdf/2412.13026">pdf</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/attn_viz2.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Learning Illumination Invariant Features for Lunar South Pole with Deep Learning</i> <br>
     <b>Georgios Georgakis</b>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/adnan_ansar/">Adnan Ansar</a> <br>
	<a>Space Imaging Workshop (<b>SIW</b>), 2024</a> <br>
	 [<a href="https://seal.ae.gatech.edu/sites/default/files/2024-09/GeorgakisGeorgios.pdf">pdf</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/pixel2elev.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Pixel to Elevation: Learning to Predict Elevation Maps at Long Range <br> using Images for Autonomous Offroad Navigation</i> <br>
     <a>Chanyoung Chung</a>, <b>Georgios Georgakis</b>, <a href="https://scholar.google.com/citations?user=s6dAIlEAAAAJ&hl=en">Patrick Spieler</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/curtis_padgett/">Curtis Padgett</a>, <br> <a>Ali Agha</a>, <a href="https://scholar.google.com/citations?user=CZ1hgVoAAAAJ&hl=en">Shehryar Khattak</a> <br>
	<a>IEEE Robotics and Automation Letters (<b>RA-L</b>), 2024</a> <br>
	 [<a href="https://arxiv.org/pdf/2401.17484.pdf">pdf</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/aeroconf2024.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Icy Moon Surface Simulation and Stereo Depth Estimation for Sampling Autonomy</i> <br>
     <a href="https://ram-bhaskara.github.io/">Ramchander Bhaskara</a>, <b>Georgios Georgakis</b>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/jeremy_nash/">Jeremy Nash</a>, <a href="https://scholar.google.com/citations?user=rYoPYlcAAAAJ&hl=en">Marissa Cameron</a>, <br>
     <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/joseph_bowkett/">Joseph Bowkett</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/adnan_ansar/">Adnan Ansar</a>, <a href="https://scholar.google.com/citations?user=paVge2QAAAAJ">Manoranjan Majji</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/paul_backes/">Paul Backes</a> <br>
	<a>IEEE Aerospace Conference (<b>AeroConf</b>), 2024</a> <br>
	 [<a href="https://arxiv.org/pdf/2401.12414.pdf">pdf</a> | <a href="https://github.com/nasa-jpl/guiss">code</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/multi-object_nav.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Unordered Navigation to Multiple Semantic Targets in Novel Environments</i> <br>
      <a href="https://bucherb.github.io/">Bernadette Bucher</a>*, <a href="https://www.linkedin.com/in/katrina-ashton/?originalSubdomain=au">Katrina Ashton</a>*, <a>Bo Wu</a>, <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>, <a>Siddharth Goel</a>, <br> <a href="https://nikolaimatni.github.io/">Nikolai Matni</a>, <b>Georgios Georgakis</b>, <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> <br>
	<a>Embodied AI Workshop at CVPR 2023</a> <br>
	 [<a href="https://embodied-ai.org/papers/2023/19.pdf">pdf</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/bridgeData_title.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets</i> <br>
     <a href="https://febert.github.io/">Frederick Ebert</a>*, <a href="https://www.linkedin.com/in/yanlai-yang/">Yanlai Yang</a>*, <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>, <a href="https://bucherb.github.io/">Bernadette Bucher</a>, 
     <br> <b>Georgios Georgakis</b>,
     <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>, <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> <br>
	<a>Robotics: Science and Systems (<b>RSS</b>), 2022</a> <br>
	* equal contribution <br>
	 [<a href="https://arxiv.org/pdf/2109.13396.pdf">pdf</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/vln_title.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Cross-modal Map Learning for Vision and Language Navigation</i> <br>
    <b>Georgios Georgakis</b>, <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>, <a href="https://wanchoo93.github.io/">Karan Wanchoo</a>, <a href="https://sdan2.github.io/">Soham Dan</a>, <br>
    <a href="https://www.miltsakaki.com/">Eleni Miltsakaki</a>, <a href="https://www.cis.upenn.edu/~danroth/">Dan Roth</a>, <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> <br>
    <a>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022</a> <br>
    [<a href="https://arxiv.org/pdf/2203.05137.pdf">pdf</a> | <a href="https://ggeorgak11.github.io/CM2-project/">project page</a> | <a href="https://github.com/ggeorgak11/CM2">code</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/nav_policy2.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Learning to Map for Active Semantic Goal Navigation</i> <br>
    <b>Georgios Georgakis</b>*, <a href="https://bucherb.github.io/">Bernadette Bucher</a>*, <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>, <br>
  Siddharth Singh, <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> <br>
	<a>International Conference on Learning Representations (<b>ICLR</b>), 2022</a> <br>
	* equal contribution <br>
	 [<a href="https://arxiv.org/pdf/2106.15648.pdf">pdf</a> | <a href="https://ggeorgak11.github.io/uncertainty-nav-project/">project page</a> | <a href="https://github.com/ggeorgak11/L2M">code</a> | <a href="https://drive.google.com/file/d/1q4gQivftFM91oCGhbJ4fV4ZS5Kdb-4AM/view?usp=sharing">video</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/UPEN_system_policy.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Uncertainty-driven Planner for Exploration and Navigation</i> <br>
    <b>Georgios Georgakis</b>, <a href="https://bucherb.github.io/">Bernadette Bucher</a>, <a href="https://www.linkedin.com/in/anton-arapin-9494b616b">Anton Arapin</a>, <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>, <br>
    <a href="https://nikolaimatni.github.io/">Nikolai Matni</a>, <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> <br>
    <a>IEEE Conference on Robotics and Automation (<b>ICRA</b>), 2022</a> <br>
	 [<a href="https://arxiv.org/pdf/2202.11907.pdf">pdf</a> | <a href="https://ggeorgak11.github.io/uncertainty-nav-project/">project page</a> | <a href="https://github.com/ggeorgak11/UPEN">code</a> | <a href="https://drive.google.com/file/d/1jqWxSfwqfV_XWSVosF0l2bszH2p0bg32/view?usp=sharing">video</a> | <a href="https://drive.google.com/file/d/1PP34iwD-t7a5R90BNobq1QLcEoI7CIcT/view?usp=sharing">slides</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/opa_title.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Object-centric Video Prediction without Annotation</i> <br>
	<a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>*, <b>Georgios Georgakis</b>*, <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a> <br>
	<a>IEEE Conference on Robotics and Automation (<b>ICRA</b>), 2021</a> <br>
	* equal contribution <br>
	 [<a href="https://arxiv.org/pdf/2105.02799.pdf">pdf</a> | <a href="https://github.com/kschmeckpeper/opa">code</a> | <a href="https://www.youtube.com/watch?v=MFdz-dwSaLg">video</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/HKMR_title.PNG" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Hierarchical Kinematic Human Mesh Recovery</i> <br>
	<b>Georgios Georgakis</b>*, <a href="https://liren2515.github.io/page/">Ren Li</a>*, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <br>
  Terrence Chen, <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a>, and <a href="http://wuziyan.com/">Ziyan Wu</a> <br>
	<a>European Conference on Computer Vision (<b>ECCV</b>), 2020</a> <br>
	* equal contribution <br>
	 [<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620749.pdf">pdf</a> | <a href="https://drive.google.com/file/d/1LHtdMyeGDIgeI9x0RFQNdrbFr6YlGt0Q/view?usp=sharing">supplementary</a> | <a href="https://drive.google.com/file/d/1wEqCMjBSPMZODoCQMFw_KoQd78yY7HMq/view?usp=sharing">long slides</a> | <a href="https://drive.google.com/file/d/1B0pXIIt5HEeV_tVJXS4iTYg2peR-zWKU/view?usp=sharing">short slides</a> | <a href="https://drive.google.com/file/d/16ZatXrWQXQDX5KBrvQSWUSM6uStFD-5F/view?usp=sharing">long video</a> | <a href="https://drive.google.com/file/d/1xco36-d1na1gcwyphC2zXmwCXALA3aER/view?usp=sharing">short video</a>]
	 </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/miccai_2020.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Robust Multi-modal 3D Patient Body Modeling</i> <br>
	Fan Yang, <a href="https://liren2515.github.io/page/">Ren Li</a>, <b>Georgios Georgakis</b>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <br>
  Terrence Chen, Haibin Ling, <a href="http://wuziyan.com/">Ziyan Wu</a> <br>
	<a>Medical Image Computing and Computer Assisted Interventions (<b>MICCAI</b>), 2020</a> <br>
	[<a href="https://www3.cs.stonybrook.edu/~hling/publication/3dbody-miccai20.pdf">pdf</a>]
	</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/title_nav.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Simultaneous Mapping and Target Driven Navigation</i> <br>
	<b>Georgios Georgakis</b>, <a href="https://yimengli46.github.io/">Yimeng Li</a>, and <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> <br>
	<a>arXiv:1911.07980</a> <br>
	 [<a href="https://arxiv.org/pdf/1911.07980.pdf">pdf</a> | <a href="https://github.com/ggeorgak11/mapping_navigation">code</a>] </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/human_pose_cai.PNG" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Towards Robust RGB-D Human Mesh Recovery</i> <br>
	Ren Li, <a href="https://www.changjiangcai.com/">Changjiang Cai</a>, <b>Georgios Georgakis</b>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <br>
  Terrence Chen, <a href="http://wuziyan.com/">Ziyan Wu</a> <br>
	<a>arXiv:1911.07383</a> <br>
	 [<a href=https://arxiv.org/pdf/1911.07383v1.pdf>pdf</a>]</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/title_2019.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Learning Local RGB-to-CAD Correspondences for Object Pose Estimation</i> <br>
	<b>Georgios Georgakis</b>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="http://wuziyan.com/">Ziyan Wu</a>, and <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> <br>
	IEEE International Conference on Computer Vision (<b>ICCV</b>), 2019 <br>
	 [<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Georgakis_Learning_Local_RGB-to-CAD_Correspondences_for_Object_Pose_Estimation_ICCV_2019_paper.pdf">pdf</a> | <a href="https://drive.google.com/file/d/1su-0PtW1rMCK2yNCyIfW3smoFPdUijOu/view?usp=sharing">slides</a> | <a href="https://drive.google.com/file/d/1fRQsIRsluVTZQ2TNC98BYvWpIiR4HfSQ/view?usp=sharing">poster</a>]</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>	

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/title_cvpr18.png" width="350" height="180" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>End-to-end Learning for Keypoint Detection and Descriptor for Pose Invariant 3D Matching</i> <br>
	<b>Georgios Georgakis</b>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="http://wuziyan.com/">Ziyan Wu</a>, <a href="https://scholar.google.com/citations?user=hFUJkFgAAAAJ&hl=en">Jan Ernst</a>, and <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> <br>
	IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2018 <br>
		[<a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Georgakis_End-to-End_Learning_of_CVPR_2018_paper.pdf">pdf</a> | <a href="https://drive.google.com/file/d/1cZzbsUDP7y4hww0BmHrT1ZKX2r2juhFj/view?usp=sharing">supplementary</a> | <a href="https://drive.google.com/file/d/1cMBSdpYL_LnuvmZPlcUe_gorY3vuGEX6/view?usp=sharing">poster</a>] </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>	

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/iros17_title.png" width="350" height="250" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Label Propagation in RGB-D Video</i> <br>
	<a href="https://cs.gmu.edu/~mreza/">Md Alimoor Reza</a>, Hui Zheng, <b>Georgios Georgakis</b>, and <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> <br>
	IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2017 <br>
		[<a href="https://drive.google.com/file/d/1eJQ99hGSgSPl5o6iK3Lk4U8qLflYjuAW/view?usp=sharing">pdf</a>] </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/synth_title.png" width="350" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Synthesizing Training Data for Object Detection in Indoor Scenes</i> <br>
	<b>Georgios Georgakis</b>, <a href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a>, <a href="http://acberg.com/">Alexander C. Berg</a>, and <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> <br>
	Robotics: Science and Systems (<b>RSS</b>), 2017 <br>
		[<a href="http://roboticsproceedings.org/rss13/p43.pdf">pdf</a> | <a href="https://drive.google.com/file/d/1XSE3jEVc3mddpIjc6Jmc3o9LmrGKRS0P/view?usp=sharing">poster</a> | <a href="https://drive.google.com/file/d/1OZnuQVBX7o4I4byRX1P6RZPOrQCWxXpj/view?usp=sharing">slides</a> | <a href="https://drive.google.com/file/d/1dXBUwFymmI3mSDlu58DktldWigE4VLZ8/view?usp=sharing">video</a> | <a href="http://cs.gmu.edu/~robot/synthesizing.html">project page</a>] </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/contact_title.png" width="350" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>A Contact Exploitative Approach to the Amazon Robotics Challenge</i> <br>
	Eadom Dessalene, <b>Georgios Georgakis</b>, <a href="https://cs.gmu.edu/~mreza/">Md Alimoor Reza</a>, <a href="https://yimengli46.github.io/">Yimeng Li</a>, <br> <a href="https://www.linkedin.com/in/yossi-ovcharik/?originalSubdomain=il">Yossi Ovcharik</a>, <a href="https://scholar.google.com/citations?user=Cb7dj-UAAAAJ&hl=en">Amir Shapiro</a>, <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a>, and <a href="http://danlofaro.com/">Daniel Lofaro</a> <br>
	Warehouse Picking Automation Workshop (<b>ICRA</b>), 2017 <br>
		[<a href="https://drive.google.com/file/d/1Lg1sPeZbK3D5jH_vOm3mbUikzLB0zJgg/view?usp=sharing">pdf</a>] </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/title_dataset.png" width="350" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>Multiview RGB-D Dataset for Object Instance Detection</i> <br>
	<b>Georgios Georgakis</b>, <a href="https://cs.gmu.edu/~mreza/">Md Alimoor Reza</a>, <a href="https://cs.gmu.edu/~amousavi/">Arsalan Mousavian</a>, <a href="https://scholar.google.com/citations?user=2-meErUAAAAJ&hl=en">Le Phi-Hung</a>, and <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> <br>
	International Conference on 3D Vision (<b>3DV</b>), 2016 <br>
		[<a href="https://arxiv.org/pdf/1609.07826.pdf">pdf</a> 
	| <a href="https://drive.google.com/file/d/1k0OJTt_i92Zk_p_Qw0MHvX-wsH7xmjsu/view?usp=sharing">poster</a> | <a href="http://cs.gmu.edu/~robot/gmu-kitchens.html">project page</a>] </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>

<table width="100%" border="0">
  <tbody>
  <tr>
    <td width="30%"><img src="images/iros16_title.png" width="350" class="papericon"></td>
  <td width="4%"> </td>
    <td width="66%"><p class="papertext"> <i>RGB-D Multiview Object Detection with Object Proposals and Shape Context</i> <br>
	<b>Georgios Georgakis</b>, <a href="https://cs.gmu.edu/~mreza/">Md Alimoor Reza</a>, and <a href="http://cs.gmu.edu/~kosecka/">Jana Kosecka</a> <br>
	IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2016 <br>
		[<a href="https://drive.google.com/file/d/1vGUZAJtrfgmEmOdkOnmn0oItCNgBM_yu/view?usp=sharing">pdf</a> | <a href="https://drive.google.com/file/d/1_EEPHQ_WCcSIdvI6eTX_J-sESb3nshi5/view?usp=sharing">poster</a> 
| <a href="https://drive.google.com/file/d/1vBNml6hScDR3MgZZcrTCuiYJn9XRvOc_/view?usp=sharing">short slides</a> | 
<a href="https://drive.google.com/file/d/1MhRhLmgvy0rXn0z-LAWhDXs4aTElRZGC/view?usp=sharing">code</a>] </td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
  </tr>
</tbody></table>

<br>


<h2>Patents Pending</h2>
<ul>
  <li>
  <i>Method and System for On-board Localization</i> <br>	
  <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/roland_brockers/">Roland Brockers</a>, <a href="https://scholar.google.co.za/citations?user=f27ESCMAAAAJ&hl=ja">Friedrich Dietsche</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/jeff_delaune/">Jeff Delaune</a>, <a href="https://pedropro.github.io/">Pedro Proença</a>, <a href="https://www-robotics.jpl.nasa.gov/who-we-are/people/robert_hewitt/">Robert Hewitt</a>, <b>Georgios Georgakis</b> <br>
  US18/312,444 filed May 2023 <br>
  Application no. <a href="https://patents.google.com/patent/US20230360547A1/en">US20230360547A1</a>
  </li>
  <br>
  <li>
  <i>Systems and Methods for Human Pose and Mesh Recovery</i> <br>	
  <a href="http://wuziyan.com/">Ziyan Wu</a>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="https://www.changjiangcai.com/">Changjiang Cai</a>, <b>Georgios Georgakis</b> <br>
  US16/995,446, filed August 2020 <br>
  Application no. <a href="https://patents.google.com/patent/US20210158028A1/en?oq=16995446">US20210158028A1</a>
  </li>
  <br>
  <li>
  <i>Systems and Methods for Human Mesh Recovery</i> <br>	
  <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="http://wuziyan.com/">Ziyan Wu</a>, <b>Georgios Georgakis</b> <br>
  US16/863,382, filed April 2020 <br>
  Application no. <a href="https://patents.google.com/patent/US20210158107A1/en?oq=16863382">US20210158107A1</a>
  </li>
  <br>
	<li>
	<i>Learning Keypoints and Matching RGB Images to CAD Models</i> <br>	
	<b>Georgios Georgakis</b>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="http://wuziyan.com/">Ziyan Wu</a>, <a href="https://scholar.google.com/citations?user=hFUJkFgAAAAJ&hl=en">Jan Ernst</a> <br>
	PCT/US2019/053827, filed September 2019 <br>
	Application no. <a href="https://patents.google.com/patent/WO2020086217A1/en?oq=WO2020086217">WO2020086217</a>
	</li>
	<br>
	<li>
	<i>Matching RGB Images to CAD Models</i> <br>	
	<b>Georgios Georgakis</b>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="http://wuziyan.com/">Ziyan Wu</a>, <a href="https://scholar.google.com/citations?user=hFUJkFgAAAAJ&hl=en">Jan Ernst</a> <br>
	PCT/US2019/040913, filed July 2019 <br>
	Application no. <a href="https://patents.google.com/patent/WO2020014170A1/en?oq=WO2020014170A1">WO2020014170A1</a>
	</li>
	<br>
	<li>
	<i>Spare Part Identification Using a Locally Learned 3D Landmark Database</i> <br>	
	<b>Georgios Georgakis</b>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="http://wuziyan.com/">Ziyan Wu</a>, <a href="https://scholar.google.com/citations?user=hFUJkFgAAAAJ&hl=en">Jan Ernst</a> <br>
	PCT/US2018/049100, filed September 2018 <br>
	Application no. <a href="https://patents.google.com/patent/WO2019094094A1/en?oq=WO2019094094A1">WO2019094094A1</a>
	</li>
	<br>
	<li>
	<i>Learning View-invariant Local Patch Representations for Pose Estimation</i> <br>	
	<b>Georgios Georgakis</b>, <a href="https://karanams.github.io/">Srikrishna Karanam</a>, <a href="http://users.umiacs.umd.edu/~varunm/">Varun Manjunatha</a>, <a href="http://www.merl.com/people/kpeng">Kuan-Chuang Peng</a>, <a href="http://wuziyan.com/">Ziyan Wu</a>, <a href="https://scholar.google.com/citations?user=hFUJkFgAAAAJ&hl=en">Jan Ernst</a> <br>
	PCT/US2018/013271, filed January 2018 <br>
	Application no. <a href="https://patents.google.com/patent/WO2019139587A1/en?oq=WO2019139587A1">WO2019139587A1</a>
	</li>
	<br>
	<li>
	<i>Training a Convolutional Neural Network using Task-irrelevant Data</i> <br>	
	<a href="http://users.umiacs.umd.edu/~varunm/">Varun Manjunatha</a>, <b>Georgios Georgakis</b>, <a href="http://www.merl.com/people/kpeng">Kuan-Chuang Peng</a>, <a href="http://wuziyan.com/">Ziyan Wu</a>, <a href="https://scholar.google.com/citations?user=hFUJkFgAAAAJ&hl=en">Jan Ernst</a> <br>
	PCT/US2017/067766, filed December 2017 <br>
	Application no. <a href="https://patents.google.com/patent/WO2019125453A1/en?oq=WO2019125453A1">WO2019125453A1</a>
	</li>
</ul>	

<br>	
	


</body></html>
